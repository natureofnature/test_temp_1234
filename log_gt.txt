INFO 09-07 16:08:07 [__init__.py:36] Available plugins for group vllm.platform_plugins:
INFO 09-07 16:08:07 [__init__.py:38] - ascend -> vllm_ascend:register
INFO 09-07 16:08:07 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 09-07 16:08:07 [__init__.py:232] Platform plugin ascend is activated
WARNING 09-07 16:08:08 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
INFO 09-07 16:08:08 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 09-07 16:08:09 [registry.py:464] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 09-07 16:08:09 [registry.py:464] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 09-07 16:08:09 [registry.py:464] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 09-07 16:08:09 [registry.py:464] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 09-07 16:08:09 [registry.py:464] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3:CustomDeepseekV3ForCausalLM.
WARNING 09-07 16:08:09 [registry.py:464] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
WARNING 09-07 16:08:09 [registry.py:464] Model architecture Qwen3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3:CustomQwen3ForCausalLM.
INFO 09-07 16:08:09 [utils.py:326] non-default args: {'model': '/mnt/weight/deepseekv3-lite-base-latest', 'trust_remote_code': True, 'max_model_len': 4096, 'tensor_parallel_size': 2, 'context_parallel_size': 4, 'enable_sequence_parallel': True, 'enable_expert_parallel': True, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 1024, 'disable_log_stats': True, 'enforce_eager': True, 'enable_chunked_prefill': True, 'additional_config': {'ascend_scheduler_config': {'enabled': True}}}
INFO 09-07 16:08:09 [config.py:240] Replacing legacy 'type' key with 'rope_type'
INFO 09-07 16:08:17 [__init__.py:742] Resolved architecture: DeepseekV3ForCausalLM
INFO 09-07 16:08:17 [__init__.py:1774] Using max model len 4096
INFO 09-07 16:08:17 [config.py:240] Replacing legacy 'type' key with 'rope_type'
INFO 09-07 16:08:17 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=1024.
INFO 09-07 16:08:17 [platform.py:156] Compilation disabled, using eager mode by default
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:18 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:18 [core.py:74] Initializing a V1 LLM engine (v0.1.dev8800+g376d98484.d20250901) with config: model='/mnt/weight/deepseekv3-lite-base-latest', speculative_config=None, tokenizer='/mnt/weight/deepseekv3-lite-base-latest', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/mnt/weight/deepseekv3-lite-base-latest, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=401312)[0;0m WARNING 09-07 16:08:18 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 640 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_1e481057'), local_subscribe_addr='ipc:///tmp/28ba0145-0404-4124-a9d2-859892643358', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8155eb0e'), local_subscribe_addr='ipc:///tmp/a7dca4b3-d3be-4b5d-a381-393b1f64a5c4', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f43ca455'), local_subscribe_addr='ipc:///tmp/27d72a15-65db-4e5c-8ff3-83b6008db008', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f541320e'), local_subscribe_addr='ipc:///tmp/21fdeef6-2b1b-44fc-a16b-5332eff0a08b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a98b386b'), local_subscribe_addr='ipc:///tmp/b4f33028-685e-4e49-a249-121d1621dde6', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3e900174'), local_subscribe_addr='ipc:///tmp/5f243368-b553-4f26-9325-363ea437b7e2', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b3609be4'), local_subscribe_addr='ipc:///tmp/6b6d8f68-d768-46cd-bbe8-0c59c659705a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c56ed163'), local_subscribe_addr='ipc:///tmp/1915617d-c63c-472d-a72a-a75a97c2cd8d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9b55c90a'), local_subscribe_addr='ipc:///tmp/f088eff6-ee66-4156-aebf-bf17765e5b50', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_94ebce4c'), local_subscribe_addr='ipc:///tmp/0625839c-c4ee-4ff9-854c-0196e83616e5', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_7ad4a398'), local_subscribe_addr='ipc:///tmp/c99c952e-0d40-4394-a3c0-387289deb877', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_604869aa'), local_subscribe_addr='ipc:///tmp/329e4fe7-c45b-489d-8052-10df704099f8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_6239d025'), local_subscribe_addr='ipc:///tmp/bab3e8fb-93f6-4a4c-abf7-39a1197e1482', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:27 [parallel_state.py:1163] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0, CP rank 0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:27 [parallel_state.py:1163] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 7, CP rank 3
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:27 [parallel_state.py:1163] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 5, CP rank 2
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:27 [parallel_state.py:1163] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 2, CP rank 1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:27 [parallel_state.py:1163] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 4, CP rank 2
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:27 [parallel_state.py:1163] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 3, CP rank 1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:27 [parallel_state.py:1163] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 6, CP rank 3
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:27 [parallel_state.py:1163] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1, CP rank 0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:28 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:28 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:28 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:28 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:28 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:28 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:28 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:28 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:28 [layer.py:828] [EP Rank 0/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7, 8->8.
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:28 [layer.py:828] [EP Rank 2/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->18, 1->19, 2->20, 3->21, 4->22, 5->23, 6->24, 7->25, 8->26.
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:28 [layer.py:828] [EP Rank 4/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->36, 1->37, 2->38, 3->39, 4->40, 5->41, 6->42, 7->43, 8->44.
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:28 [layer.py:828] [EP Rank 7/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->63, 1->64, 2->65, 3->66, 4->67, 5->68, 6->69, 7->70, 8->71.
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:28 [layer.py:828] [EP Rank 3/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->27, 1->28, 2->29, 3->30, 4->31, 5->32, 6->33, 7->34, 8->35.
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:28 [layer.py:828] [EP Rank 6/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->54, 1->55, 2->56, 3->57, 4->58, 5->59, 6->60, 7->61, 8->62.
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:28 [layer.py:828] [EP Rank 1/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->9, 1->10, 2->11, 3->12, 4->13, 5->14, 6->15, 7->16, 8->17.
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:28 [layer.py:828] [EP Rank 5/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->45, 1->46, 2->47, 3->48, 4->49, 5->50, 6->51, 7->52, 8->53.
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:34 [default_loader.py:267] Loading weights took 4.73 seconds
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:34 [default_loader.py:267] Loading weights took 4.91 seconds
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:34 [default_loader.py:267] Loading weights took 5.02 seconds
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:34 [default_loader.py:267] Loading weights took 5.34 seconds
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:34 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:35 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:35 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:35 [default_loader.py:267] Loading weights took 5.88 seconds
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:35 [default_loader.py:267] Loading weights took 5.89 seconds
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:35 [default_loader.py:267] Loading weights took 6.01 seconds
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:35 [default_loader.py:267] Loading weights took 6.28 seconds
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:36 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:36 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:36 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:36 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:36 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:40 [worker_v1.py:185] Available memory: 48155133747, total memory: 65796046848
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:40 [worker_v1.py:185] Available memory: 47897711616, total memory: 65787658240
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:40 [worker_v1.py:185] Available memory: 47907197952, total memory: 65787658240
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:40 [worker_v1.py:185] Available memory: 48152940339, total memory: 65796046848
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:40 [worker_v1.py:185] Available memory: 48161125171, total memory: 65796046848
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:40 [worker_v1.py:185] Available memory: 48164583219, total memory: 65796046848
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:40 [worker_v1.py:185] Available memory: 47912370176, total memory: 65787658240
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:40 [worker_v1.py:185] Available memory: 47445005312, total memory: 65787658240
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:849] GPU KV cache size: 1,372,800 tokens
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2681.25x
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:849] GPU KV cache size: 1,393,280 tokens
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2721.25x
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:849] GPU KV cache size: 1,385,856 tokens
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2706.75x
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:849] GPU KV cache size: 1,393,536 tokens
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2721.75x
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:849] GPU KV cache size: 1,386,240 tokens
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2707.50x
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:849] GPU KV cache size: 1,393,280 tokens
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2721.25x
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:849] GPU KV cache size: 1,386,112 tokens
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2707.25x
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:849] GPU KV cache size: 1,393,536 tokens
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:40 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2721.75x
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:41 [core.py:215] init engine (profile, create kv cache, warmup model) took 4.31 seconds
[1;36m(EngineCore_0 pid=401312)[0;0m WARNING 09-07 16:08:41 [core.py:109] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[1;36m(EngineCore_0 pid=401312)[0;0m INFO 09-07 16:08:41 [platform.py:156] Compilation disabled, using eager mode by default
INFO 09-07 16:08:41 [llm.py:283] Supported_tasks: ['generate']
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1169] num_tokens for request 0: 2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1169] num_tokens for request 0: 2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1169] num_tokens for request 0: 2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1169] num_tokens for request 0: 2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1169] num_tokens for request 0: 2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1169] num_tokens for request 0: 2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1169] num_tokens for request 0: 2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1169] num_tokens for request 0: 2497
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[626]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[626]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[626]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1249] $$$$$ slot mapping [2504]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1249] $$$$$ slot mapping [2504]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1249] $$$$$ slot mapping [2504]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[626]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[626]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[626]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[626]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1249] $$$$$ slot mapping [2504]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1249] $$$$$ slot mapping [2504]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1249] $$$$$ slot mapping [2504]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1249] $$$$$ slot mapping [2504]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] start_index:0, kv_save_start:1565, num_save_token_rank:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] shape_left:(313,),shape_right:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] start_index:0, kv_save_start:626, num_save_token_rank:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] shape_left:(313,),shape_right:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] start_index:0, kv_save_start:939, num_save_token_rank:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] shape_left:(313,),shape_right:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[626]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1249] $$$$$ slot mapping [2504]
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] start_index:0, kv_save_start:1878, num_save_token_rank:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] shape_left:(313,),shape_right:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] start_index:0, kv_save_start:0, num_save_token_rank:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] shape_left:(313,),shape_right:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] start_index:0, kv_save_start:313, num_save_token_rank:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] shape_left:(313,),shape_right:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] start_index:0, kv_save_start:1252, num_save_token_rank:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] shape_left:(313,),shape_right:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] start_index:0, kv_save_start:2191, num_save_token_rank:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1086] shape_left:(313,),shape_right:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([626], dtype=torch.int32) of self.cpu_len:tensor([626,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([626], dtype=torch.int32) of self.cpu_len:tensor([626,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([626], dtype=torch.int32) of self.cpu_len:tensor([626,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([626], dtype=torch.int32) of self.cpu_len:tensor([626,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([626], dtype=torch.int32) of self.cpu_len:tensor([626,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([626], dtype=torch.int32) of self.cpu_len:tensor([626,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([626], dtype=torch.int32) of self.cpu_len:tensor([626,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([626], dtype=torch.int32) of self.cpu_len:tensor([626,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:313
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([626], dtype=torch.int32), query_lens:tensor([626], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([626], dtype=torch.int32), query_lens:tensor([626], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:41 [mla_v1.py:364] ********* here in build, chunked prefill enabled:False, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([626], dtype=torch.int32), query_lens:tensor([626], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([626], dtype=torch.int32), query_lens:tensor([626], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([626], dtype=torch.int32), query_lens:tensor([626], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([626], dtype=torch.int32), query_lens:tensor([626], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([626], dtype=torch.int32), query_lens:tensor([626], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:41 [mla_v1.py:364] ********* here in build, chunked prefill enabled:False, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:41 [mla_v1.py:364] ********* here in build, chunked prefill enabled:False, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:41 [mla_v1.py:364] ********* here in build, chunked prefill enabled:False, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:41 [mla_v1.py:364] ********* here in build, chunked prefill enabled:False, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([626], dtype=torch.int32), query_lens:tensor([626], dtype=torch.int32)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:41 [mla_v1.py:364] ********* here in build, chunked prefill enabled:False, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:41 [mla_v1.py:364] ********* here in build, chunked prefill enabled:False, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:41 [mla_v1.py:364] ********* here in build, chunked prefill enabled:False, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m WARNING 09-07 16:08:42 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401329)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401349)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401319)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP0 pid=401339)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([313, 16, 128]),q_rope_shape:torch.Size([313, 16, 64]), attn_out_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401334)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401324)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([313])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:994] q_full_idx shape:torch.Size([626])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401352)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([626, 16, 128]),q_nope_shape_select:torch.Size([313, 16, 128]), q_pe shape:torch.Size([626, 16, 64]),k_nope shape:torch.Size([2504, 16, 128]),k_pe shape:torch.Size([2504, 16, 64])
[1;36m(EngineCore_0 pid=401312)[0;0m [1;36m(VllmWorker TP1 pid=401344)[0;0m INFO 09-07 16:08:42 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([313, 16, 128]), out_tail_shape:torch.Size([313, 16, 128])
TTFT: 931.4115047454834 ms
req_num: 0
[['1991', '1993', '1995', '1997', '']] -> Generated text: '199'
Token ids: [1357]

end.
ERROR 09-07 16:08:43 [core_client.py:562] Engine core proc EngineCore_0 died unexpectedly, shutting down client.
