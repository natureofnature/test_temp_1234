INFO 09-05 16:05:27 [__init__.py:36] Available plugins for group vllm.platform_plugins:
INFO 09-05 16:05:27 [__init__.py:38] - ascend -> vllm_ascend:register
INFO 09-05 16:05:27 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 09-05 16:05:27 [__init__.py:232] Platform plugin ascend is activated
WARNING 09-05 16:05:28 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
INFO 09-05 16:05:29 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 09-05 16:05:30 [registry.py:464] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 09-05 16:05:30 [registry.py:464] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 09-05 16:05:30 [registry.py:464] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 09-05 16:05:30 [registry.py:464] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 09-05 16:05:30 [registry.py:464] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3:CustomDeepseekV3ForCausalLM.
WARNING 09-05 16:05:30 [registry.py:464] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
WARNING 09-05 16:05:30 [registry.py:464] Model architecture Qwen3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3:CustomQwen3ForCausalLM.
INFO 09-05 16:05:30 [utils.py:326] non-default args: {'model': '/mnt/weight/deepseekv3-lite-base-latest', 'trust_remote_code': True, 'max_model_len': 4096, 'tensor_parallel_size': 2, 'context_parallel_size': 4, 'enable_sequence_parallel': True, 'enable_expert_parallel': True, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 1024, 'disable_log_stats': True, 'enforce_eager': True, 'enable_chunked_prefill': True, 'additional_config': {'ascend_scheduler_config': {'enabled': True}}}
INFO 09-05 16:05:30 [config.py:240] Replacing legacy 'type' key with 'rope_type'
INFO 09-05 16:05:38 [__init__.py:742] Resolved architecture: DeepseekV3ForCausalLM
INFO 09-05 16:05:38 [__init__.py:1774] Using max model len 4096
INFO 09-05 16:05:38 [config.py:240] Replacing legacy 'type' key with 'rope_type'
INFO 09-05 16:05:38 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=1024.
INFO 09-05 16:05:38 [platform.py:156] Compilation disabled, using eager mode by default
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:05:39 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:05:39 [core.py:74] Initializing a V1 LLM engine (v0.1.dev8800+g376d98484.d20250901) with config: model='/mnt/weight/deepseekv3-lite-base-latest', speculative_config=None, tokenizer='/mnt/weight/deepseekv3-lite-base-latest', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/mnt/weight/deepseekv3-lite-base-latest, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=301568)[0;0m WARNING 09-05 16:05:39 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 640 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:05:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_857ce100'), local_subscribe_addr='ipc:///tmp/ab879ad0-051a-4d80-9945-c13a12f4d31a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:05:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f89f39e7'), local_subscribe_addr='ipc:///tmp/229d1226-3aa2-47d4-a06e-c2532ad0104d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:05:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3d4f291a'), local_subscribe_addr='ipc:///tmp/e6184522-8581-405c-a922-5bdc54743d74', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:05:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0cd9e533'), local_subscribe_addr='ipc:///tmp/fd8606ad-9366-4f80-b975-f4ca3e21d950', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:05:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6b6047a1'), local_subscribe_addr='ipc:///tmp/71153c9e-af5b-489c-9c9c-389931d56723', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:05:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c42d0251'), local_subscribe_addr='ipc:///tmp/4055effb-d0d0-4088-a877-64ef9977fc7c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:05:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ef07801d'), local_subscribe_addr='ipc:///tmp/d3532fc0-f423-49f2-8205-268531c2e282', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:05:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2155cbf0'), local_subscribe_addr='ipc:///tmp/0276d900-e831-4fdb-949c-3d201c7a5c5b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:05:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_018e1b80'), local_subscribe_addr='ipc:///tmp/6ec22f29-9724-409d-9a1f-9de30a47534f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:05:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_795a7208'), local_subscribe_addr='ipc:///tmp/f9883d4e-23a0-4774-89b2-ebc700644797', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:05:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_0449088d'), local_subscribe_addr='ipc:///tmp/c1e81bf7-6f9b-481c-bdf9-7a194b1c0732', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:05:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_d188e1ac'), local_subscribe_addr='ipc:///tmp/6880c083-2bd0-46ef-8b01-5b86c6bdc582', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:05:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_959bda83'), local_subscribe_addr='ipc:///tmp/849311f1-957c-4b65-a0fa-310e0c4beefa', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:05:48 [parallel_state.py:1163] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1, CP rank 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:05:48 [parallel_state.py:1163] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 7, CP rank 3
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:05:48 [parallel_state.py:1163] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 5, CP rank 2
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:05:48 [parallel_state.py:1163] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 3, CP rank 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:05:48 [parallel_state.py:1163] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 2, CP rank 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:05:48 [parallel_state.py:1163] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 4, CP rank 2
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:05:48 [parallel_state.py:1163] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 6, CP rank 3
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:05:48 [parallel_state.py:1163] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0, CP rank 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:05:48 [model_runner_v1.py:2444] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:05:48 [model_runner_v1.py:2444] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:05:48 [model_runner_v1.py:2444] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:05:48 [model_runner_v1.py:2444] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:05:48 [model_runner_v1.py:2444] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:05:48 [model_runner_v1.py:2444] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:05:49 [model_runner_v1.py:2444] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:05:49 [model_runner_v1.py:2444] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:05:49 [layer.py:828] [EP Rank 4/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->36, 1->37, 2->38, 3->39, 4->40, 5->41, 6->42, 7->43, 8->44.
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:05:49 [layer.py:828] [EP Rank 1/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->9, 1->10, 2->11, 3->12, 4->13, 5->14, 6->15, 7->16, 8->17.
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:05:49 [layer.py:828] [EP Rank 3/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->27, 1->28, 2->29, 3->30, 4->31, 5->32, 6->33, 7->34, 8->35.
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:05:49 [layer.py:828] [EP Rank 7/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->63, 1->64, 2->65, 3->66, 4->67, 5->68, 6->69, 7->70, 8->71.
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:05:49 [layer.py:828] [EP Rank 5/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->45, 1->46, 2->47, 3->48, 4->49, 5->50, 6->51, 7->52, 8->53.
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:05:49 [layer.py:828] [EP Rank 6/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->54, 1->55, 2->56, 3->57, 4->58, 5->59, 6->60, 7->61, 8->62.
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:05:49 [layer.py:828] [EP Rank 2/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->18, 1->19, 2->20, 3->21, 4->22, 5->23, 6->24, 7->25, 8->26.
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:05:49 [layer.py:828] [EP Rank 0/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7, 8->8.
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:05:55 [default_loader.py:267] Loading weights took 4.85 seconds
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:05:55 [default_loader.py:267] Loading weights took 5.10 seconds
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:05:55 [default_loader.py:267] Loading weights took 5.06 seconds
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:05:55 [default_loader.py:267] Loading weights took 5.27 seconds
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:05:55 [default_loader.py:267] Loading weights took 5.35 seconds
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:05:56 [default_loader.py:267] Loading weights took 5.52 seconds
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:05:56 [default_loader.py:267] Loading weights took 5.61 seconds
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:05:56 [default_loader.py:267] Loading weights took 6.26 seconds
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:05:56 [model_runner_v1.py:2474] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:05:56 [model_runner_v1.py:2474] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:05:56 [model_runner_v1.py:2474] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:05:56 [model_runner_v1.py:2474] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:05:56 [model_runner_v1.py:2474] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:05:57 [model_runner_v1.py:2474] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:05:57 [model_runner_v1.py:2474] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:05:57 [model_runner_v1.py:2474] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:01 [worker_v1.py:185] Available memory: 47749918720, total memory: 65787658240
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:01 [worker_v1.py:185] Available memory: 48006592307, total memory: 65796046848
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:01 [worker_v1.py:185] Available memory: 48009232179, total memory: 65796046848
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:01 [worker_v1.py:185] Available memory: 48004576051, total memory: 65796046848
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:01 [worker_v1.py:185] Available memory: 47277376512, total memory: 65787658240
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:01 [worker_v1.py:185] Available memory: 47751508992, total memory: 65787658240
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:01 [worker_v1.py:185] Available memory: 47752414208, total memory: 65787658240
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:01 [worker_v1.py:185] Available memory: 47999440691, total memory: 65796046848
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:849] GPU KV cache size: 1,367,936 tokens
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2671.75x
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:849] GPU KV cache size: 1,389,056 tokens
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2713.00x
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:849] GPU KV cache size: 1,381,632 tokens
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2698.50x
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:849] GPU KV cache size: 1,388,800 tokens
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2712.50x
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:849] GPU KV cache size: 1,381,632 tokens
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2698.50x
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:849] GPU KV cache size: 1,389,056 tokens
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2713.00x
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:849] GPU KV cache size: 1,381,632 tokens
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2698.50x
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:849] GPU KV cache size: 1,388,928 tokens
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2712.75x
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:01 [core.py:215] init engine (profile, create kv cache, warmup model) took 4.40 seconds
[1;36m(EngineCore_0 pid=301568)[0;0m WARNING 09-05 16:06:02 [core.py:109] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[1;36m(EngineCore_0 pid=301568)[0;0m INFO 09-05 16:06:02 [platform.py:156] Compilation disabled, using eager mode by default
INFO 09-05 16:06:02 [llm.py:283] Supported_tasks: ['generate']
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] start_index:0, kv_save_start:512, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] start_index:0, kv_save_start:128, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] start_index:0, kv_save_start:0, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] start_index:0, kv_save_start:768, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] start_index:0, kv_save_start:640, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] start_index:0, kv_save_start:896, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] start_index:0, kv_save_start:384, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] start_index:0, kv_save_start:256, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:02 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:02 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:02 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:02 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:02 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:02 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:02 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:02 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L1/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L2/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L3/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L4/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L5/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L6/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L7/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L8/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L9/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L11/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L12/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L13/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L14/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L15/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L16/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L17/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L18/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L19/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L20/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L21/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L22/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L23/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L24/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L25/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L25/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L26/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L26/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L26/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L27/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L28/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L29/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:384, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:0, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:512, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:128, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:768, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:256, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:896, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:640, num_save_token_rank:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:128, tail_len:128
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L1/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L1/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L2/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L2/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L3/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L3/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L3/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L4/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L4/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L5/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L5/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L6/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L6/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L6/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L6/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L7/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L8/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L9/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L9/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L9/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L11/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L11/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L12/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L12/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L13/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L13/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L13/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L14/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L14/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L15/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L15/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L15/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L16/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L17/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L17/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L18/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L19/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L20/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L21/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L22/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L23/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L24/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L25/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L26/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L26/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L27/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L28/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L28/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L29/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L29/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:114, num_save_token_rank:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:57, num_save_token_rank:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:0, num_save_token_rank:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:228, num_save_token_rank:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:171, num_save_token_rank:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:285, num_save_token_rank:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:399, num_save_token_rank:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] start_index:0, kv_save_start:342, num_save_token_rank:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:57, tail_len:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:57, tail_len:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:57, tail_len:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:57, tail_len:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:57, tail_len:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:57, tail_len:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:57, tail_len:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [model_runner_v1.py:1305] >>>>> q_req_offset:0, head_len:57, tail_len:57
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:349] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:353] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:365] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L0/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L1/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L1/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L1/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L2/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L3/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L3/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L3/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L4/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L4/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L4/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L5/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L5/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L6/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L6/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L7/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L7/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L7/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L8/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L8/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L8/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L9/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L10/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L11/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L11/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L11/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L12/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L12/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L12/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L12/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L12/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L12/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L13/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L13/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L13/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L13/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L13/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L14/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L14/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L14/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L14/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L15/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L15/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L15/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L15/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L16/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L16/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L16/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L16/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L16/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L17/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L17/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L17/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L18/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L18/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L18/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L19/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L19/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L19/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L19/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L19/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L20/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L20/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L20/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L20/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L21/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L21/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L21/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L22/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L22/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L22/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L22/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L22/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L23/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L23/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L23/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L23/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L23/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L24/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L24/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L24/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L24/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L25/tp0/cp1-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L25/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L25/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L25/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L26/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L26/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L26/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L27/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L27/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L27/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L27/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L27/tp1/cp3-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:03 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L28/tp0/cp3-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L28/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L28/tp1/cp0-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:03 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m WARNING 09-05 16:06:04 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m WARNING 09-05 16:06:04 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m WARNING 09-05 16:06:04 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m WARNING 09-05 16:06:04 [mla_v1.py:1429] [KVDBG] hook failed: [Errno 2] No such file or directory: './kv_debug/groundtruth/manifest_gt.json.tmp' -> './kv_debug/groundtruth/manifest_gt.json'
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:04 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:04 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301610)[0;0m INFO 09-05 16:06:04 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301580)[0;0m INFO 09-05 16:06:04 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:04 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301605)[0;0m INFO 09-05 16:06:04 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:04 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301585)[0;0m INFO 09-05 16:06:04 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:04 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L29/tp0/cp0-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:04 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L29/tp1/cp1-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:04 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L29/tp0/cp2-sp0/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:04 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301575)[0;0m INFO 09-05 16:06:04 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:04 [kv_debug.py:266] [KVDBG] saved GT kv to ./kv_debug/groundtruth/L29/tp1/cp2-sp1/kv.pkl
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:04 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301590)[0;0m INFO 09-05 16:06:04 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:04 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP0 pid=301595)[0;0m INFO 09-05 16:06:04 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:04 [mla_v1.py:1088] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:04 [mla_v1.py:985] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:04 [mla_v1.py:986] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=301568)[0;0m [1;36m(VllmWorker TP1 pid=301600)[0;0m INFO 09-05 16:06:04 [mla_v1.py:987] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
TTFT: 1631.615161895752 ms
prompt:1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137 139 141 143 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175 177 179 181 183 185 187 189 191 193 195 197 199 201 203 205 207 209 211 213 215 217 219 221 223 225 227 229 231 233 235 237 239 241 243 245 247 249 251 253 255 257 259 261 263 265 267 269 271 273 275 277 279 281 283 285 287 289 291 293 295 297 299 301 303 305 307 309 311 313 315 317 319 321 323 325 327 329 331 333 335 337 339 341 343 345 347 349 351 353 355 357 359 361 363 365 367 369 371 373 375 377 379 381 383 385 387 389 391 393 395 397 399 401 403 405 407 409 411 413 415 417 419 421 423 425 427 429 431 433 435 437 439 441 443 445 447 449 451 453 455 457 459 461 463 465 467 469 471 473 475 477 479 481 483 485 487 489 491 493 495 497 499 501 503 505 507 509 511 513 515 517 519 521 523 525 527 529 531 533 535 537 539 541 543 545 547 549 551 553 555 557 559 561 563 565 567 569 571 573 575 577 579 581 583 585 587 589 591 593 595 597 599 601 603 605 607 609 611 613 615 617 619 621 623 625 627 629 631 633 635 637 639 641 643 645 647 649 651 653 655 657 659 661 663 665 667 669 671 673 675 677 679 681 683 685 687 689 691 693 695 697 699 701 703 705 707 709 711 713 715 717 719 721 723 725 727 729 731 733 735 737 739 741 743 745 747 749 751 753 755 757 759 761 763 765 767 769 771 773 775 777 779 781 783 785 787 789 791 793 795 797 799 801 803 805 807 809 811 813 815 817 819 821 823 825 827 829 831 833 835 837 839 841 843 845 847 849 851 853 855 857 859 861 863 865 867 869 871 873 875 877 879 881 883 885 887 889 891 893 895 897 899 901 903 905 907 909 911 913 915 917 919 921 923 925 927 929 931 933 935 937 939 941 943 945 947 949 951 953 955 957 959 961 963 965 967 969 971 973 975 977 979 981 983 985 987 989 991 993 995 997 999 1001 1003 1005 1007 1009 1011 1013 1015 1017 1019 1021 1023 1025 1027 1029 1031 1033 1035 1037 1039 1041 1043 1045 1047 1049 1051 1053 1055 1057 1059 1061 1063 1065 1067 1069 1071 1073 1075 1077 1079 1081 1083 1085 1087 1089 1091 1093 1095 1097 1099 1101 1103 1105 1107 1109 1111 1113 1115 1117 1119 1121 1123 1125 1127 1129 1131 1133 1135 1137 1139 1141 1143 1145 1147 1149 1151 1153 1155 1157 1159 1161 1163 1165 1167 1169 1171 1173 1175 1177 1179 1181 1183 1185 1187 1189 1191 1193 1195 1197 1199 1201 1203 1205 1207 1209 1211 1213 1215 1217 1219 1221 1223 1225 1227 1229 1231 1233 1235 1237 1239 1241 1243 1245 1247 1249 1251 1253 1255 1257 1259 1261 1263 1265 1267 1269 1271 1273 1275 1277 1279 1281 1283 1285 1287 1289 1291 1293 1295 1297 1299 1301 1303 1305 1307 1309 1311 1313 1315 1317 1319 1321 1323 1325 1327 1329 1331 1333 1335 1337 1339 1341 1343 1345 1347 1349 1351 1353 1355 1357 1359 1361 1363 1365 1367 1369 1371 1373 1375 1377 1379 1381 1383 1385 1387 1389 1391 1393 1395 1397 1399 1401 1403 1405 1407 1409 1411 1413 1415 1417 1419 1421 1423 1425 1427 1429 1431 1433 1435 1437 1439 1441 1443 1445 1447 1449 1451 1453 1455 1457 1459 1461 1463 1465 1467 1469 1471 1473 1475 1477 1479 1481 1483 1485 1487 1489 1491 1493 1495 1497 1499 1501 1503 1505 1507 1509 1511 1513 1515 1517 1519 1521 1523 1525 1527 1529 1531 1533 1535 1537 1539 1541 1543 1545 1547 1549 1551 1553 1555 1557 1559 1561 1563 1565 1567 1569 1571 1573 1575 1577 1579 1581 1583 1585 1587 1589 1591 1593 1595 1597 1599 1601 1603 1605 1607 1609 1611 1613 1615 1617 1619 1621 1623 1625 1627 1629 1631 1633 1635 1637 1639 1641 1643 1645 1647 1649 1651 1653 1655 1657 1659 1661 1663 1665 1667 1669 1671 1673 1675 1677 1679 1681 1683 1685 1687 1689 1691 1693 1695 1697 1699 1701 1703 1705 1707 1709 1711 1713 1715 1717 1719 1721 1723 1725 1727 1729 1731 1733 1735 1737 1739 1741 1743 1745 1747 1749 1751 1753 1755 1757 1759 1761 1763 1765 1767 1769 1771 1773 1775 1777 1779 1781 1783 1785 1787 1789 1791 1793 1795 1797 1799 1801 1803 1805 1807 1809 1811 1813 1815 1817 1819 1821 1823 1825 1827 1829 1831 1833 1835 1837 1839 1841 1843 1845 1847 1849 1851 1853 1855 1857 1859 1861 1863 1865 1867 1869 1871 1873 1875 1877 1879 1881 1883 1885 1887 1889 1891 1893 1895 1897 1899 1901 1903 1905 1907 1909 1911 1913 1915 1917 1919 1921 1923 1925 1927 1929 1931 1933 1935 1937 1939 1941 1943 1945 1947 1949 1951 1953 1955 1957 1959 1961 1963 1965 1967 1969 1971 1973 1975 1977 1979 1981 1983 1985 1987 1989 1991 1993 1995 1997 
req_num: 0
Generated text: '197'
Token ids: [2254]

end.
ERROR 09-05 16:06:05 [core_client.py:562] Engine core proc EngineCore_0 died unexpectedly, shutting down client.
