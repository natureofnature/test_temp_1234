INFO 09-07 16:08:51 [__init__.py:36] Available plugins for group vllm.platform_plugins:
INFO 09-07 16:08:51 [__init__.py:38] - ascend -> vllm_ascend:register
INFO 09-07 16:08:51 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 09-07 16:08:51 [__init__.py:232] Platform plugin ascend is activated
WARNING 09-07 16:08:53 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
INFO 09-07 16:08:53 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 09-07 16:08:54 [registry.py:464] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 09-07 16:08:54 [registry.py:464] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 09-07 16:08:54 [registry.py:464] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 09-07 16:08:54 [registry.py:464] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 09-07 16:08:54 [registry.py:464] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3:CustomDeepseekV3ForCausalLM.
WARNING 09-07 16:08:54 [registry.py:464] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
WARNING 09-07 16:08:54 [registry.py:464] Model architecture Qwen3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3:CustomQwen3ForCausalLM.
INFO 09-07 16:08:54 [utils.py:326] non-default args: {'model': '/mnt/weight/deepseekv3-lite-base-latest', 'trust_remote_code': True, 'max_model_len': 4096, 'tensor_parallel_size': 2, 'context_parallel_size': 4, 'enable_sequence_parallel': True, 'enable_expert_parallel': True, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 1024, 'disable_log_stats': True, 'enforce_eager': True, 'enable_chunked_prefill': True, 'additional_config': {'ascend_scheduler_config': {'enabled': True}}}
INFO 09-07 16:08:54 [config.py:240] Replacing legacy 'type' key with 'rope_type'
INFO 09-07 16:09:02 [__init__.py:742] Resolved architecture: DeepseekV3ForCausalLM
INFO 09-07 16:09:02 [__init__.py:1774] Using max model len 4096
INFO 09-07 16:09:02 [config.py:240] Replacing legacy 'type' key with 'rope_type'
INFO 09-07 16:09:02 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=1024.
INFO 09-07 16:09:02 [platform.py:156] Compilation disabled, using eager mode by default
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:03 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:03 [core.py:74] Initializing a V1 LLM engine (v0.1.dev8800+g376d98484.d20250901) with config: model='/mnt/weight/deepseekv3-lite-base-latest', speculative_config=None, tokenizer='/mnt/weight/deepseekv3-lite-base-latest', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/mnt/weight/deepseekv3-lite-base-latest, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=405158)[0;0m WARNING 09-07 16:09:03 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 640 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_3abb0661'), local_subscribe_addr='ipc:///tmp/d90d4948-6ef2-4e58-afd3-79ac50b31a19', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b9ed0e56'), local_subscribe_addr='ipc:///tmp/9ba89847-7970-4018-b6c8-42dd118aef72', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2ac9d5ee'), local_subscribe_addr='ipc:///tmp/0ef0428f-156c-47bc-b061-2dd3b305f3c3', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_19a431a5'), local_subscribe_addr='ipc:///tmp/2bafca5c-4504-423a-bb7d-fb4fc9200a04', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f7c93b69'), local_subscribe_addr='ipc:///tmp/4fbc9226-bfb1-4ac8-8725-567938340b4b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_16a76883'), local_subscribe_addr='ipc:///tmp/7911e811-a3fb-493d-9141-60db2db82e18', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7d820fd9'), local_subscribe_addr='ipc:///tmp/0a2a93dc-f236-4b25-af0e-cff6b0997142', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b1f964d7'), local_subscribe_addr='ipc:///tmp/d286099f-fb2f-47f7-ab83-a9940c2f3041', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fb3443df'), local_subscribe_addr='ipc:///tmp/92865868-f02d-4bcb-8ed9-554bb5f7136f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_2179ec02'), local_subscribe_addr='ipc:///tmp/06ec8c35-e43b-4098-af21-41b95fbdbffe', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_8097c534'), local_subscribe_addr='ipc:///tmp/2c1f075f-1f97-48bb-8836-0c910d84c719', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_f14ccb02'), local_subscribe_addr='ipc:///tmp/3aaaf06f-493c-44d4-adf0-9c652b9d0b43', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_6b4f3229'), local_subscribe_addr='ipc:///tmp/ef3c2c96-7949-45e0-be72-86907969041a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:12 [parallel_state.py:1163] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0, CP rank 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:12 [parallel_state.py:1163] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 2, CP rank 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:12 [parallel_state.py:1163] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1, CP rank 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:12 [parallel_state.py:1163] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 5, CP rank 2
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:12 [parallel_state.py:1163] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 6, CP rank 3
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:12 [parallel_state.py:1163] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 4, CP rank 2
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:12 [parallel_state.py:1163] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 3, CP rank 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:12 [parallel_state.py:1163] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 7, CP rank 3
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:12 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:12 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:12 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:12 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:12 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:12 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:12 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:12 [model_runner_v1.py:2430] Starting to load model /mnt/weight/deepseekv3-lite-base-latest...
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:13 [layer.py:828] [EP Rank 1/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->9, 1->10, 2->11, 3->12, 4->13, 5->14, 6->15, 7->16, 8->17.
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:13 [layer.py:828] [EP Rank 2/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->18, 1->19, 2->20, 3->21, 4->22, 5->23, 6->24, 7->25, 8->26.
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:13 [layer.py:828] [EP Rank 4/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->36, 1->37, 2->38, 3->39, 4->40, 5->41, 6->42, 7->43, 8->44.
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:13 [layer.py:828] [EP Rank 5/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->45, 1->46, 2->47, 3->48, 4->49, 5->50, 6->51, 7->52, 8->53.
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:13 [layer.py:828] [EP Rank 6/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->54, 1->55, 2->56, 3->57, 4->58, 5->59, 6->60, 7->61, 8->62.
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:13 [layer.py:828] [EP Rank 7/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->63, 1->64, 2->65, 3->66, 4->67, 5->68, 6->69, 7->70, 8->71.
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:13 [layer.py:828] [EP Rank 0/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7, 8->8.
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:13 [layer.py:828] [EP Rank 3/8] Expert parallelism is enabled. Local/global number of experts: 9/72. Experts local to global index map: 0->27, 1->28, 2->29, 3->30, 4->31, 5->32, 6->33, 7->34, 8->35.
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:18 [default_loader.py:267] Loading weights took 4.61 seconds
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:18 [default_loader.py:267] Loading weights took 4.52 seconds
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:18 [default_loader.py:267] Loading weights took 4.53 seconds
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:19 [default_loader.py:267] Loading weights took 4.76 seconds
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:19 [default_loader.py:267] Loading weights took 4.73 seconds
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:19 [default_loader.py:267] Loading weights took 4.76 seconds
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:19 [default_loader.py:267] Loading weights took 4.75 seconds
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:19 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:19 [default_loader.py:267] Loading weights took 5.24 seconds
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:19 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:19 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:20 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:20 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:20 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:20 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:20 [model_runner_v1.py:2460] Loading model weights took 8.9590 GB
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:24 [worker_v1.py:185] Available memory: 48002365235, total memory: 65796046848
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:24 [worker_v1.py:185] Available memory: 47999774515, total memory: 65796046848
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:24 [worker_v1.py:185] Available memory: 47754204160, total memory: 65787658240
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:24 [worker_v1.py:185] Available memory: 48011302707, total memory: 65796046848
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:24 [worker_v1.py:185] Available memory: 48007811891, total memory: 65796046848
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:24 [worker_v1.py:185] Available memory: 47759560704, total memory: 65787658240
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:24 [worker_v1.py:185] Available memory: 47291806720, total memory: 65787658240
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:24 [worker_v1.py:185] Available memory: 47744586752, total memory: 65787658240
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:849] GPU KV cache size: 1,368,320 tokens
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2672.50x
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:849] GPU KV cache size: 1,388,800 tokens
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2712.50x
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:849] GPU KV cache size: 1,381,376 tokens
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2698.00x
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:849] GPU KV cache size: 1,389,184 tokens
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2713.25x
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:849] GPU KV cache size: 1,381,888 tokens
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2699.00x
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:849] GPU KV cache size: 1,388,928 tokens
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2712.75x
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:849] GPU KV cache size: 1,381,760 tokens
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2698.75x
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:849] GPU KV cache size: 1,389,056 tokens
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 2713.00x
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:24 [core.py:215] init engine (profile, create kv cache, warmup model) took 4.03 seconds
[1;36m(EngineCore_0 pid=405158)[0;0m WARNING 09-07 16:09:25 [core.py:109] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[1;36m(EngineCore_0 pid=405158)[0;0m INFO 09-07 16:09:25 [platform.py:156] Compilation disabled, using eager mode by default
INFO 09-07 16:09:25 [llm.py:283] Supported_tasks: ['generate']
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] start_index:0, kv_save_start:128, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[0], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] start_index:0, kv_save_start:768, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] start_index:0, kv_save_start:896, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] start_index:0, kv_save_start:384, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] start_index:0, kv_save_start:640, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] start_index:0, kv_save_start:256, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] start_index:0, kv_save_start:0, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] start_index:0, kv_save_start:512, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:25 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:25 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 1024
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[1024], num_scheduled_tokens:[256]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [1024]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:384, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:128, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:896, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:256, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:0, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:640, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:768, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:512, num_save_token_rank:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(128,),shape_right:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([256], dtype=torch.int32) of self.cpu_len:tensor([256,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:128
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([256], dtype=torch.int32), query_lens:tensor([256], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([128, 16, 128]),q_rope_shape:torch.Size([128, 16, 64]), attn_out_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([256])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([256, 16, 128]),q_nope_shape_select:torch.Size([128, 16, 128]), q_pe shape:torch.Size([256, 16, 64]),k_nope shape:torch.Size([1024, 16, 128]),k_pe shape:torch.Size([1024, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([128, 16, 128]), out_tail_shape:torch.Size([128, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1130] *************** scheduler total scheduled tokens:449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1169] num_tokens for request 0: 449
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1234] self.input_batch.num_computed_tokens_cpu[:num_reqs]:[2048], num_scheduled_tokens:[114]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1249] $$$$$ slot mapping [456]
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:57, num_save_token_rank:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:171, num_save_token_rank:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:342, num_save_token_rank:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 0, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:0, num_save_token_rank:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 3, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:399, num_save_token_rank:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:228, num_save_token_rank:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 2, sp = 1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:285, num_save_token_rank:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] ++++++++, i = 0, cp = 1, sp = 0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] num_computed_and_new_tokens_batch shape:(1, 4, 2)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] start_index:0, kv_save_start:114, num_save_token_rank:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1086] shape_left:(57,),shape_right:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1298] >>>>> seq_lens:tensor([114], dtype=torch.int32) of self.cpu_len:tensor([114,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [model_runner_v1.py:1302] >>>>> q_req_offset:0, chunk len:57
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:348] num_computed_tokens_cpu:tensor([0], dtype=torch.int32), seq_lens:tensor([114], dtype=torch.int32), query_lens:tensor([114], dtype=torch.int32)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:352] ********* here in build, num prefills:1
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:364] ********* here in build, chunked prefill enabled:True, max_context_len_cpu:0
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] KV snapshot failed: Overloaded torch operator invoked from Python failed to match any schema:
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load.out(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False, Tensor(a!) key, Tensor(b!) value) -> (Tensor(a!), Tensor(b!))
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] atb::npu_paged_cache_load() takes 4 positional argument(s) but 5 was/were given.  Declaration: atb::npu_paged_cache_load(Tensor key_cache, Tensor value_cache, Tensor block_table, Tensor context_lens, *, Tensor? seq_starts=None, bool cumsum=False) -> (Tensor, Tensor)
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m WARNING 09-07 16:09:26 [mla_v1.py:1467] 
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405170)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405200)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405180)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP1 pid=405190)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405175)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:1097] +++ in attn with mask/nomask, q_nope shape:torch.Size([57, 16, 128]),q_rope_shape:torch.Size([57, 16, 64]), attn_out_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405165)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405195)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] ============> q_tail_idx shape:torch.Size([57])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:994] q_full_idx shape:torch.Size([114])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:995] ============> here before meta prefill, q_nope shape:torch.Size([114, 16, 128]),q_nope_shape_select:torch.Size([57, 16, 128]), q_pe shape:torch.Size([114, 16, 64]),k_nope shape:torch.Size([456, 16, 128]),k_pe shape:torch.Size([456, 16, 64])
[1;36m(EngineCore_0 pid=405158)[0;0m [1;36m(VllmWorker TP0 pid=405185)[0;0m INFO 09-07 16:09:26 [mla_v1.py:996] ============> here before meta prefill, out_head_shape:torch.Size([57, 16, 128]), out_tail_shape:torch.Size([57, 16, 128])
TTFT: 1467.322587966919 ms
req_num: 0
[['1991', '1993', '1995', '1997', '']] -> Generated text: '197'
Token ids: [2254]

end.
ERROR 09-07 16:09:27 [core_client.py:562] Engine core proc EngineCore_0 died unexpectedly, shutting down client.
