INFO 09-08 17:04:59 [__init__.py:36] Available plugins for group vllm.platform_plugins:
INFO 09-08 17:04:59 [__init__.py:38] - ascend -> vllm_ascend:register
INFO 09-08 17:04:59 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 09-08 17:04:59 [__init__.py:232] Platform plugin ascend is activated
WARNING 09-08 17:05:00 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
INFO 09-08 17:05:00 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 09-08 17:05:01 [registry.py:464] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 09-08 17:05:01 [registry.py:464] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 09-08 17:05:01 [registry.py:464] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 09-08 17:05:01 [registry.py:464] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 09-08 17:05:01 [registry.py:464] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3:CustomDeepseekV3ForCausalLM.
WARNING 09-08 17:05:01 [registry.py:464] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
WARNING 09-08 17:05:01 [registry.py:464] Model architecture Qwen3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3:CustomQwen3ForCausalLM.
INFO 09-08 17:05:01 [utils.py:326] non-default args: {'model': '/home/DeepSeek-V2-Lite', 'trust_remote_code': True, 'max_model_len': 4096, 'tensor_parallel_size': 2, 'context_parallel_size': 2, 'enable_sequence_parallel': True, 'enable_expert_parallel': True, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 1024, 'disable_log_stats': True, 'enforce_eager': True, 'enable_chunked_prefill': True, 'additional_config': {'ascend_scheduler_config': {'enabled': True}}}
INFO 09-08 17:05:01 [config.py:240] Replacing legacy 'type' key with 'rope_type'
INFO 09-08 17:05:10 [__init__.py:742] Resolved architecture: DeepseekV2ForCausalLM
INFO 09-08 17:05:10 [__init__.py:1774] Using max model len 4096
INFO 09-08 17:05:10 [config.py:240] Replacing legacy 'type' key with 'rope_type'
INFO 09-08 17:05:10 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=1024.
INFO 09-08 17:05:10 [platform.py:156] Compilation disabled, using eager mode by default
[1;36m(EngineCore_0 pid=569269)[0;0m INFO 09-08 17:05:10 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=569269)[0;0m INFO 09-08 17:05:10 [core.py:74] Initializing a V1 LLM engine (v0.1.dev8800+g376d98484.d20250901) with config: model='/home/DeepSeek-V2-Lite', speculative_config=None, tokenizer='/home/DeepSeek-V2-Lite', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/DeepSeek-V2-Lite, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=569269)[0;0m WARNING 09-08 17:05:10 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 640 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_0 pid=569269)[0;0m INFO 09-08 17:05:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_271d5745'), local_subscribe_addr='ipc:///tmp/91b37574-68ed-42b4-a533-7fe26c111840', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP0 pid=569286)[0;0m INFO 09-08 17:05:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a3e55054'), local_subscribe_addr='ipc:///tmp/e8abd265-7592-460b-8d14-e124f0374e36', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP0 pid=569276)[0;0m INFO 09-08 17:05:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dc30f745'), local_subscribe_addr='ipc:///tmp/e160d6b5-5922-4ef1-b8b8-fa4bb62a9bbc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP1 pid=569291)[0;0m INFO 09-08 17:05:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_233855f9'), local_subscribe_addr='ipc:///tmp/0fe01323-d8b0-4ce0-9d64-a115a9e0ce36', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP1 pid=569281)[0;0m INFO 09-08 17:05:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_44178dfb'), local_subscribe_addr='ipc:///tmp/fb9ed0b9-c861-4fbb-a28d-30f516bf34fa', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP0 pid=569286)[0;0m INFO 09-08 17:05:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_f85d0a83'), local_subscribe_addr='ipc:///tmp/81f28275-09bc-423c-9244-7ffad2b7cf41', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP0 pid=569276)[0;0m INFO 09-08 17:05:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_56b17109'), local_subscribe_addr='ipc:///tmp/1fe6a225-eb16-470d-8805-ccdc8b20a8b2', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP0 pid=569286)[0;0m INFO 09-08 17:05:16 [parallel_state.py:1163] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 2, CP rank 1
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP0 pid=569276)[0;0m INFO 09-08 17:05:16 [parallel_state.py:1163] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0, CP rank 0
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP1 pid=569281)[0;0m INFO 09-08 17:05:16 [parallel_state.py:1163] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1, CP rank 0
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP1 pid=569291)[0;0m INFO 09-08 17:05:16 [parallel_state.py:1163] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 3, CP rank 1
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP0 pid=569286)[0;0m INFO 09-08 17:05:16 [model_runner_v1.py:2430] Starting to load model /home/DeepSeek-V2-Lite...
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP1 pid=569281)[0;0m INFO 09-08 17:05:16 [model_runner_v1.py:2430] Starting to load model /home/DeepSeek-V2-Lite...
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP1 pid=569291)[0;0m INFO 09-08 17:05:16 [model_runner_v1.py:2430] Starting to load model /home/DeepSeek-V2-Lite...
[1;36m(EngineCore_0 pid=569269)[0;0m [1;36m(VllmWorker TP0 pid=569276)[0;0m INFO 09-08 17:05:16 [model_runner_v1.py:2430] Starting to load model /home/DeepSeek-V2-Lite...
